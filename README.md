# eeg-mcp-project
# ğŸ§  MCP - EEG Data Processing & Local RAG Retrieval App

This project contains:
**EEG Data Processing MCP (Model Context Protocol) Server** using BrainFlow.
**Local RAG (Retrieval-Augmented Generation)** system that leverages LangChain, Chroma VectorDB, and Ollama for contextual document-based Q&A for EEG.
---

## ğŸ—‚ï¸ Project Structure

eeg-mcp-project/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ brainflow_server.py # MCP APIs (read, visualize, filter, features)
â”‚ â”œâ”€â”€ rag_server.py # RAG API server using Ollama LLM + Vector DB
â”‚ â”œâ”€â”€ vectorstore.py # Vector DB Population script
â”‚ â””â”€â”€ .env # API keys & env variables (ignored in git)
â”œâ”€â”€ uploads/ # Temp uploads folder
â”œâ”€â”€ data/ # EDF EEG data files
â”œâ”€â”€ vectorstore/ # Persisted Chroma Vector DB files
â”œâ”€â”€ requirements.txt # Python dependencies list
â”œâ”€â”€ .gitignore # Ignore node_modules, .env, cache files
â””â”€â”€ README.md # This file

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

git clone https://github.com/your-username/eeg-mcp-project.git
cd eeg-mcp-project

### 2ï¸âƒ£ Setup Python Virtual Environment
python -m venv .venv
source .venv/bin/activate  # Mac/Linux
# .venv\Scripts\activate   # Windows

### 3ï¸âƒ£ Install Python Dependencies
pip install -r requirement.txt

### 4ï¸âƒ£ Install & Start Ollama (Local LLM)
brew install ollama  # macOS via Homebrew
ollama serve         # Start Ollama server (runs on 127.0.0.1:11434)
ollama pull mistral  # Pull Mistral model for RAG inference

### 5ï¸âƒ£ Populate Vector Database (One-time Step)
python backend/vectorstore.py

### 6ï¸âƒ£ Start MCP Backend Server
python backend/brainflow_server.py

### 7ï¸âƒ£ Start RAG Server (LangChain + Ollama)
python backend/rag_server.py

### 8ï¸âƒ£ (Optional) Node.js Forwarding Server
cd frontend/
npm install
npm start

### âœ… Features Breakdown
1. EEG MCP Server (BrainFlow)

/read-edf â€“ Read EEG EDF files.

/visualize-edf â€“ Visualize EEG signals (plot).

/filter-edf â€“ Apply bandpass filtering to EEG signals.

/features-edf â€“ (Optional) Extract band power features.

2. RAG Knowledge Assistant (Ollama + Chroma)
Uses Chroma VectorDB to store documents (docs/brainflow_notes.txt etc.).

Runs Mistral model via Ollama for local LLM answering.

API: /mcp/query â€“ Ask questions about EEG concepts, filtering, brainflow usage.

3. Unified API Gateway (server.js)
MCP endpoints (read, visualize, filter)

RAG endpoint (/mcp/query) for knowledge QA.

Acts as a single entry point but routes to independent backends.


### ğŸ”„ Restarting Workflow
Whenever you return to work:
# Activate environment
source .venv/bin/activate

# Start Ollama server
ollama serve

# Start MCP server
python backend/brainflow_server.py

# Start RAG server
python backend/rag_server.py

# (Optional) Populate VectorDB again if updated
python backend/vectorstore.py

# (Optional) Run Node.js server
cd frontend/
npm start

### ğŸ¯ Goal of This Project
"EEG Signal Processing (MCP) + Local Doc-based Assistant (RAG) for EEG"

### âœ… Important Notes

MCP uses BrainFlow SDK for EEG Data acquisition & filtering.

RAG system uses Ollama (local LLM inference) with VectorDB (Chroma).

No OpenAI API keys needed (runs locally via Ollama).